{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "TJJtp0AXkgub",
    "outputId": "cf65e60a-8f7f-47fa-a7fe-e045d0b65df9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "IS_COLAB = not os.path.exists(\"hmm\")\n",
    "ABSOLUTE_PATH = \"/content/drive/My Drive/MLSS2019/tutorials/\" if IS_COLAB else \"\"\n",
    "if IS_COLAB:\n",
    "    !pip install -q simplejson soundfile\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=True)\n",
    "    import sys\n",
    "    sys.path.append(ABSOLUTE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XypXNJLOdiq1"
   },
   "source": [
    "# Speech tutorial\n",
    "-------------------------------------------------\n",
    "- This tutorial includes a bit of signal processing and a couple of simple speech recognizers, both HMM-based and neural (RNN-based).\n",
    "- The tutorial is heavily based on exercises created by Shane Settle for TTIC's speech processing course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcSazvMiQS2i"
   },
   "source": [
    "# Part 1: A bit of signal processing\n",
    "-------------------------------------------------\n",
    "- This part is standalone and not required in order to run the speech recognition experiments.  \n",
    "- You are encouraged to at least run through the Part 1 code, to get a sense of what speech signals \"look\" like and what a speech recognizer sees at its input.  Then, if you want to do a deep dive, you can spend a good chunk of time on the \"things to do\" below; otherwise, just move on to Part 2.\n",
    "- This part takes a single speech signal and runs it through a classic feature extraction pipeline that computes mel frequency cepstral coefficients (MFCCs).\n",
    "- Human-engineered features like MFCCs are still useful for small-data settings, but are no longer commonly used in large-scale speech systems.  \n",
    "- For our purposes in this tutorial, computing MFCCs also allows us to quickly experiment with different sizes of feature vectors and try to understand how large a representation we really need for speech.\n",
    "- Note:  This is a machine learning summer school and not a signal processing summer school.  You are not expected to understand every detail in this part, but if things really don't make sense, please ask questions!\n",
    "\n",
    "### Things to do:\n",
    "-----------\n",
    "- Run through the steps and see if you can figure out why things sound/look as they do\n",
    "- How does the recovered signal at the end compare to the original recording?  Any idea why it sounds that way?\n",
    "- Re-run with smaller numbers of feature dimensions by lowering 'ncoeffs'. What's the smallest number such that the re-synthesized utterance is still intelligible?\n",
    "- Play with the different settings of <em>size</em> (window size), <em>step</em> (window shift), <em>nfilters</em> (the number of filters in the mel-filterbank), and <em>ncoeffs</em> and observe their effect on the visualizations. Note that the process for inversion can be fragile, so it is best to try powers of 2 (e.g. <em>size</em> = [64, 128, 256, 512, 1024, ...] and <em>step</em> = [8, 16, 32, 64, 128, ...]).  If you've gotten this far and some of the signal processing is making sense (or you already knew some signal processing), see if you can figure out what values of these parameters make sense for speech recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ec9AW90CQS2a"
   },
   "outputs": [],
   "source": [
    "# Some setup\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "PLOT_CONFIG = { 'interpolation': \"nearest\", 'aspect': \"auto\", 'cmap': \"Greys\" }\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "from numpy.fft import fft, ifft\n",
    "from scipy.fftpack import dct, idct\n",
    "\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgu-a9Z-QS2j"
   },
   "outputs": [],
   "source": [
    "# The main signal processing functions we will use\n",
    "\n",
    "def pre_emphasis(x):\n",
    "    \"\"\"\n",
    "    Applies pre-emphasis to the signal:  Balances the spectrum by increasing the \n",
    "    amplitudes of high-frequency components and decreasing the amplitudes of \n",
    "    lower-frequency components (much like turning up the treble and turning\n",
    "    down the bass)\n",
    "    ------\n",
    "    :in:\n",
    "    x, array of samples\n",
    "    ------\n",
    "    :out:\n",
    "    y, array of samples\n",
    "    \"\"\"\n",
    "    y = np.append(x[0], x[1:] - 0.97 * x[:-1])\n",
    "    \n",
    "    return y\n",
    "    \n",
    "def hamming(n):\n",
    "    \"\"\"\n",
    "    Hamming window for weighting samples within an analysis window.\n",
    "    ------\n",
    "    :in: \n",
    "    n, window size\n",
    "    ------\n",
    "    :out: \n",
    "    win, array of weights to apply along window\n",
    "    \"\"\"\n",
    "    win = 0.54 - .46 * np.cos(2 * np.pi * np.arange(n) / (n - 1))\n",
    "    \n",
    "    return win\n",
    "    \n",
    "def windowing(x, size, step):\n",
    "    \"\"\"\n",
    "    Window and stack signal into overlapping frames.\n",
    "    ------\n",
    "    :in:\n",
    "    x, array of samples\n",
    "    size, window size in number of samples (Note: this should be a power of 2)\n",
    "    step, window shift in number of samples\n",
    "    ------\n",
    "    :out:\n",
    "    frames, 2d-array of frames with shape (number of windows, window size)\n",
    "    \"\"\"\n",
    "    xpad = np.append(x, np.zeros((size - len(x) % size)))\n",
    "    \n",
    "    T = (len(xpad) - size) // step\n",
    "    frames = np.stack([xpad[t * step:t * step + size] for t in range(T)])\n",
    "    \n",
    "    return frames\n",
    "    \n",
    "def discrete_fourier_transform(x):\n",
    "    \"\"\"\n",
    "    Compute the discrete fourier transform for each frame of windowed signal x.\n",
    "    Typically, we talk about performing the DFT on short-time windows\n",
    "    (often referred to as the Short-Time Fourier Transform). Here, the input\n",
    "    is a 2d-array with shape (window size,  number of windows). We want to\n",
    "    perform the DFT on each of these windows.\n",
    "    --------\n",
    "    :in: \n",
    "    x, 2d-array of frames with shape (window size, number of windows)\n",
    "    --------\n",
    "    :out:\n",
    "    X, 2d-array of complex spectrum after DFT applied to each window of x\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(x)\n",
    "    indices = np.arange(n)\n",
    "    M = np.exp(-2j * np.pi * np.outer(indices, indices) / n)\n",
    "    return np.dot(M, x)\n",
    "\n",
    "    \n",
    "def fast_fourier_transform(x):\n",
    "    \"\"\"\n",
    "    Fast-fourier transform. Efficient algorithm for computing the DFT.\n",
    "    --------\n",
    "    :in: \n",
    "    x, 2d-array of frames with shape (window size, number of windows)\n",
    "    --------\n",
    "    :out:\n",
    "    X, 2d-array of complex spectrum after DFT applied to each window of x\n",
    "    \"\"\"\n",
    "    fft_size = len(x)\n",
    "\n",
    "    if fft_size <= 16:\n",
    "        X = discrete_fourier_transform(x)\n",
    "    \n",
    "    else:\n",
    "        indices = np.arange(fft_size)\n",
    "        even = fast_fourier_transform(x[::2])\n",
    "        odd = fast_fourier_transform(x[1::2])\n",
    "        m = np.exp(-2j * np.pi * indices / fft_size).reshape(-1, 1)\n",
    "        X = np.concatenate([even + m[:fft_size // 2] * odd, even + m[fft_size // 2:] * odd])\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def mel_filterbank(nfilters, fft_size, sample_rate):\n",
    "    \"\"\"\n",
    "    A set of filters that warps the speech spectrum in a similar way to the human ear.\n",
    "    --------\n",
    "    :in: \n",
    "    nfilters, number of filters\n",
    "    fft_size, window size over which fft is performed\n",
    "    sample_rate, sampling rate of the signal\n",
    "    --------\n",
    "    :out:\n",
    "    mel_filter, 2d-array of (fft_size / 2, nfilters) \n",
    "    mel_inv_filter, 2d-array of (nfilters, fft_size / 2) used to invert later\n",
    "    melpoints, 1d-array of frequencies converted to the mel scale\n",
    "    \"\"\"\n",
    "    freq2mel = lambda f: 2595. * np.log10(1 + f / 700.)\n",
    "    mel2freq = lambda m: 700. * (10**(m / 2595.) - 1)\n",
    "\n",
    "    lowfreq = 0\n",
    "    highfreq = sample_rate // 2\n",
    "\n",
    "    lowmel = freq2mel(lowfreq)\n",
    "    highmel = freq2mel(highfreq)\n",
    "\n",
    "    melpoints = np.linspace(lowmel, highmel, 1 + nfilters + 1)\n",
    "\n",
    "    # convert from freq to fft bin number\n",
    "    fft_bins = ((fft_size + 1) * mel2freq(melpoints) // sample_rate).astype(np.int32)\n",
    "\n",
    "    filterbank = np.zeros((nfilters, fft_size // 2))\n",
    "    for j in range(nfilters):\n",
    "        for i in range(fft_bins[j], fft_bins[j + 1]):\n",
    "            filterbank[j, i] = (i - fft_bins[j]) / (fft_bins[j + 1] - fft_bins[j])\n",
    "        for i in range(fft_bins[j + 1], fft_bins[j + 2]):\n",
    "            filterbank[j, i] = (fft_bins[j + 2] - i) / (fft_bins[j + 2] - fft_bins[j + 1])\n",
    "\n",
    "    mel_filter = filterbank.T / filterbank.sum(axis=1).clip(1e-16)\n",
    "    mel_inv_filter = filterbank\n",
    "\n",
    "    return mel_filter, mel_inv_filter, melpoints\n",
    "    \n",
    "    \n",
    "def inv_spectrogram(X_s, size, step, n_iter=15):\n",
    "    \"\"\"\n",
    "    Invert from a spectrogram back to an audible waveform.\n",
    "    \"\"\"\n",
    "    \n",
    "    def find_offset(a, b):\n",
    "        corrs = np.convolve(a - a.mean(), b[::-1] - b.mean())\n",
    "        corrs[:len(b) // 2] = -1e12\n",
    "        corrs[-len(b) // 2:] = -1e12\n",
    "        return corrs.argmax() - len(a)\n",
    "\n",
    "    def iterate(X, iteration):\n",
    "        T, n = X.shape\n",
    "        size = n // 2\n",
    "\n",
    "        x = np.zeros((T * step + size))\n",
    "        window_sum = np.zeros((T * step + size))\n",
    "\n",
    "        est_start = size // 2 - 1\n",
    "        est_stop = est_start + size\n",
    "\n",
    "        for t in range(T):\n",
    "            x_start = t * step\n",
    "            x_stop = x_start + size\n",
    "\n",
    "            est = ifft(X[t].real + 0j if iteration == 0 else X[t]).real[::-1]            \n",
    "            if t > 0 and x_stop - step > x_start and est_stop - step > est_start:\n",
    "                offset = find_offset(x[x_start:x_stop - step], est[est_start:est_stop - step])\n",
    "            else:\n",
    "                offset = 0\n",
    "                \n",
    "            x[x_start:x_stop] += est[est_start - offset:est_stop - offset] * hamming(size)\n",
    "            window_sum[x_start:x_stop] += hamming(size)\n",
    "\n",
    "        return x.real / window_sum.clip(1e-12)\n",
    "\n",
    "    X_s = np.concatenate([X_s, X_s[:, ::-1]], axis=1)\n",
    "    reg = np.max(X_s) / 1e8\n",
    "\n",
    "    X_best = iterate(deepcopy(X_s), 0)\n",
    "    for i in range(1, n_iter):\n",
    "        X_best = windowing(X_best, size, step) * hamming(size)\n",
    "        est = fast_fourier_transform(X_best.T).T\n",
    "        phase = est / np.maximum(reg, np.abs(est))\n",
    "        X_best = iterate(X_s * phase[:len(X_s)], i)\n",
    "    \n",
    "    return np.real(X_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "id": "-0LDzXbcQS2p",
    "outputId": "fae63602-7c2b-4886-bd8c-c3bfb94bb2b6"
   },
   "outputs": [],
   "source": [
    "# Load the speech signal (waveform)\n",
    "signal, fs = sf.read(ABSOLUTE_PATH + \"aurora_FMS_15739A.wav\")\n",
    "Audio(data=signal, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "id": "ulPmursIQS2s",
    "outputId": "bc92c69b-bb48-465e-b614-b8174d36dfec"
   },
   "outputs": [],
   "source": [
    "# Pre-emphasize the signal\n",
    "Audio(data=pre_emphasis(signal), rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "fpdD5WbrQS2v",
    "outputId": "bb21160f-bf56-40c4-b3e7-59f0c1382f58"
   },
   "outputs": [],
   "source": [
    "# Just for fun, confirm how much faster the fast Fourier transform is than a naive discrete Fourier transform\n",
    "\n",
    "print(\"\\nWindow size = 128\")\n",
    "frames128 = windowing(signal, 128, 64) * hamming(128)\n",
    "print(\"Timing vanilla DFT\")\n",
    "%timeit -n 50 discrete_fourier_transform(frames128.T).T\n",
    "print(\"Timing FFT\")\n",
    "%timeit -n 50 fast_fourier_transform(frames128.T).T\n",
    "\n",
    "print(\"\\nWindow size = 512\")\n",
    "frames512 = windowing(signal, 512, 256) * hamming(512)\n",
    "print(\"Timing vanilla DFT\")\n",
    "%timeit -n 50 discrete_fourier_transform(frames512.T).T\n",
    "print(\"Timing FFT\")\n",
    "%timeit -n 50 fast_fourier_transform(frames512.T).T\n",
    "\n",
    "print(\"\\nWindow size = 1024\")\n",
    "frames1024 = windowing(signal, 1024, 512) * hamming(1024)\n",
    "print(\"Timing vanilla DFT\")\n",
    "%timeit -n 50 discrete_fourier_transform(frames1024.T).T\n",
    "print(\"Timing FFT\")\n",
    "%timeit -n 50 fast_fourier_transform(frames1024.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "EClUr-Y4QS2y",
    "outputId": "435bbaff-48a7-4f53-b697-28da74233c75"
   },
   "outputs": [],
   "source": [
    "# Compute the spectrogram of the signal\n",
    "\n",
    "# Set some parameters\n",
    "size = 128 # window size for the FFT\n",
    "step = size // 2 # time between consecutive windows\n",
    "nfilters = 26 # number of mel frequency channels\n",
    "ncoeffs = 13 # number of feature dimensions (MFCCs) to keep\n",
    "\n",
    "# Pre-emphasize\n",
    "pre_emphasized_signal = pre_emphasis(signal)\n",
    "\n",
    "# Window the signal\n",
    "frames = windowing(pre_emphasized_signal, size, step) * hamming(size)\n",
    "\n",
    "# Compute the (complex) spectrum\n",
    "spectrum = fast_fourier_transform(frames.T).T\n",
    "spectrum = spectrum[:, :size // 2] # only need to keep half since it's symmetric\n",
    "\n",
    "# Compute the spectrum magnitude (this is typically what is meant by \"spectrogram\")\n",
    "magnitude = np.abs(spectrum)\n",
    "\n",
    "# Get the spectrum power\n",
    "power = magnitude**2 / size\n",
    "\n",
    "# Visualize log spectrogram\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18,4))\n",
    "cax = ax.matshow(20*np.log10(magnitude.clip(1e-12)).T, origin='lower', **PLOT_CONFIG)\n",
    "fig.colorbar(cax, label='dB')\n",
    "plt.title('log spectrogram (dB)')\n",
    "plt.xlabel('# Frames')\n",
    "plt.ylabel('Hz')\n",
    "ixs = np.arange(0, size // 2, 2)\n",
    "freqs = ixs * fs // size\n",
    "plt.yticks(ixs, freqs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pc2O93BZQS24"
   },
   "outputs": [],
   "source": [
    "# Warp the spectrogram frequency axis to one that is more similar to what the human ear computes\n",
    "\n",
    "# Generate the mel filters and mel inverse filters\n",
    "mel_filter, mel_inv_filter, melpoints = mel_filterbank(nfilters, size, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "wUEHhjyxQS2_",
    "outputId": "18dbd170-2207-4045-8703-a9e428823922"
   },
   "outputs": [],
   "source": [
    "# Visualize the warped spectrogram (often called a \"log mel spectrogram\")\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18,4))\n",
    "cax = ax.matshow(20*np.log10(magnitude.dot(mel_filter).clip(1e-16)).T, origin='lower', **PLOT_CONFIG)\n",
    "fig.colorbar(cax, label='dB')\n",
    "plt.title('log mel spectrogram (dB)')\n",
    "plt.xlabel('# Frames')\n",
    "plt.ylabel('Mel frequency units')\n",
    "ixs = np.arange(0, nfilters, 2)\n",
    "plt.yticks(ixs, map(int, melpoints[1::2]));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "xLzu0pJbQS3E",
    "outputId": "1ddc7f96-96d5-44c9-e0e0-c063281296c3"
   },
   "outputs": [],
   "source": [
    "# Finally, compute the feature vectors (MFCCs)\n",
    "\n",
    "# Apply mel warping filters to power spectrum and take log10\n",
    "log_mel_fbank = np.log10(power.dot(mel_filter).clip(1e-16))\n",
    "\n",
    "# Compute MFCCs using discrete cosine transform (DCT)\n",
    "\"\"\"\n",
    "Note: similarly to the DFT, the DCT is used to decompose a finite discrete-time \n",
    "vector into a sum of scaled and shifted (real-valued) cosine functions\n",
    "\"\"\"\n",
    "mfccs = dct(log_mel_fbank, type=2, axis=1, norm='ortho')\n",
    "\n",
    "# Keep a subset of the feature dimensions (cepstral coefficients)\n",
    "mfccs = mfccs[:,:ncoeffs]\n",
    "\n",
    "# Visualize the MFCCs\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18,4))\n",
    "cax = ax.matshow(mfccs.T, origin='lower', **PLOT_CONFIG)\n",
    "fig.colorbar(cax)\n",
    "plt.title('MFCCs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "id": "KfPfufbwQS3M",
    "outputId": "c1d885ae-f7ef-46b2-8603-d64501c59fff"
   },
   "outputs": [],
   "source": [
    "# Invert the whole process and listen to the reconstructed speech, \n",
    "# to get a sense of what information we have lost in this representation.\n",
    "\n",
    "# Invert from MFCCs back to waveform, starting with inverting the DCT\n",
    "recovered_log_mel_fbank = idct(mfccs, type=2, n=nfilters, axis=1, norm='ortho')\n",
    "\n",
    "# Exponentiate log and invert mel warping\n",
    "recovered_power = (10**recovered_log_mel_fbank).dot(mel_inv_filter)\n",
    "\n",
    "# Invert mel warping of spectrogram\n",
    "recovered_magnitude = np.sqrt(recovered_power * size)\n",
    "\n",
    "# Finally invert the spectrogram to get back a waveform\n",
    "recovered_signal = inv_spectrogram(recovered_magnitude, size, step)\n",
    "Audio(data=recovered_signal, rate=fs)  \n",
    "\n",
    "# (Note: we have not inverted the preemphasis, so we are hearing a bit more of \n",
    "# the higher and less of the lower frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGozBBkBZCi1"
   },
   "source": [
    "# Part 2: Recognizing 2-digit numbers with HMMs\n",
    "-------------------------------------------------\n",
    "- In this part you will train and test a simple hidden Markov model-based speech recognizer that recognizes 1- or 2-digit numbers.  \n",
    "- Each word is a digit in the range 0-9, where 0 is pronounced \"oh\" (not \"zero\").\n",
    "- Each word is modeled as a separate HMM, with the same number of states for each word.\n",
    "- The observation distribution is a single Gaussian in each state.\n",
    "- (We are doing a very simple recognition task, using very simple models, so that we can get reasonable performance within the time span of the tutorial.)\n",
    "\n",
    "### Things to do:\n",
    "-----------\n",
    "- Run through the steps and make sure things are making sense.\n",
    "- Experiment with different values for the number of states in the single-digit model.\n",
    "- Try changing other aspects: parameter initialization, convergence criteria, HMM structure, or any other aspect that you find interesting.\n",
    "- See if you can improve the word error rate through such changes.\n",
    "- Note that there is no separate development (tuning) set, just a train set and a test set.  So for purposes of this exercise, we are tuning on the test set (the horror!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eS9Jzok0Wosl"
   },
   "outputs": [],
   "source": [
    "# Some setup\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import editdistance\n",
    "import os\n",
    "np.seterr(divide='ignore') # masks log(0) errors\n",
    "from hmm.multiple import FullGaussianHMM\n",
    "from hmm.single import GaussianHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "NWJ6dgkyWos2",
    "outputId": "03931831-a742-4c8a-da1c-16ad2a7e969e"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Single-digit HMM recognizer\n",
    "\"\"\"\n",
    "data_single_digit = np.load(ABSOLUTE_PATH + \"hmm/data/mfccs_single.npz\", allow_pickle=True)\n",
    "\n",
    "n_states = 15  # number of states per word\n",
    "n_dims = 13  # number feature dimensions per input frame\n",
    "n_iter = 0  # single iteration\n",
    "model = dict()\n",
    "digits = range(10)\n",
    "    \n",
    "# Training\n",
    "for digit in digits:\n",
    "    print(\"Training HMM for digit %d\" % digit)\n",
    "    Xtrain_digit = [x for x, y in zip(data_single_digit[\"Xtrain\"], data_single_digit[\"Ytrain\"]) if y == digit]\n",
    "    model[digit] = GaussianHMM(n_states=n_states, n_dims=n_dims)\n",
    "    model[digit].init_gaussian_params(Xtrain_digit)\n",
    "    model[digit].init_hmm_params()\n",
    "    \n",
    "    # Run Baum-Welch (EM) training\n",
    "    for i in range(n_iter):\n",
    "        print(\"starting iteration {}...\".format(i+1))\n",
    "        model[digit].train(Xtrain_digit)\n",
    "\n",
    "# Testing\n",
    "print(\"\\nTesting...\")\n",
    "accuracy = np.zeros(10)\n",
    "confusion = np.zeros((10, 10))\n",
    "for x, y in zip(data_single_digit[\"Xtest\"], data_single_digit[\"Ytest\"]):\n",
    "    T = len(x)\n",
    "\n",
    "    # Since this is single-word recognition, we will simply score the\n",
    "    # observed input with each HMM, and pick the one with the highest score.\n",
    "    # We could use the forward, backward, or Viterbi algorithm for scoring;\n",
    "    # here we are using Viterbi.\n",
    "    scores = []\n",
    "    for digit in digits:\n",
    "        log_pi = np.log(model[digit].pi)\n",
    "        log_A = np.log(model[digit].A)\n",
    "        log_B = model[digit].get_emissions(x)\n",
    "        _, log_prob = model[digit].viterbi(log_pi, log_A, log_B)\n",
    "        scores.append(log_prob)\n",
    "\n",
    "    top_digit, top_log_prob = sorted(zip(digits, scores), key=lambda x: -x[1])[0]\n",
    "    confusion[y, top_digit] += 1.\n",
    "\n",
    "accuracy = np.diag(confusion) / confusion.sum(axis=1)\n",
    "\n",
    "print(\"accuracy:  overall {:.4f}, per-digit {}\".format(accuracy.mean(), accuracy))\n",
    "\n",
    "# Save the model\n",
    "with open(ABSOLUTE_PATH + \"hmm/single_digit_model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zv1VqRF5fcqo"
   },
   "source": [
    "### Well that was easy. \n",
    "-----------\n",
    "- Almost 100% accuracy.\n",
    "- Let's see how things go when we move to 2-digit sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "8-llfxRAWos9",
    "outputId": "de0e1ccb-c923-479e-e199-62d2700054e2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multiple-digit HMM recognizer\n",
    "\"\"\"\n",
    "data_multiple_digit = np.load(ABSOLUTE_PATH + \"hmm/data/mfccs_multiple.npz\", allow_pickle=True)\n",
    "\n",
    "# Initialize the model with the parameters from the single-digit recognizer.\n",
    "full_model = FullGaussianHMM(data_multiple_digit[\"Xtrain\"], ABSOLUTE_PATH + \"hmm/single_digit_model.pkl\")\n",
    "\n",
    "n_iter = 1\n",
    "\n",
    "# Train\n",
    "print(\"Training HMM\")\n",
    "for i in range(n_iter):\n",
    "    print(\"starting iteration {}...\".format(i + 1))\n",
    "    full_model.train(data_multiple_digit[\"Xtrain\"], data_multiple_digit[\"Ytrain\"])\n",
    "\n",
    "# Test\n",
    "print(\"Testing HMM\")\n",
    "test_wer = full_model.test(data_multiple_digit[\"Xtest\"], data_multiple_digit[\"Ytest\"])\n",
    "print(\"{:.2f}% word error rate\".format(test_wer * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfDKLc0XiAbj"
   },
   "source": [
    "# Part 3: Recognizing 2-digit numbers with RNNs + attention\n",
    "-------------------------------------------------\n",
    "- In this part you will train and test a simple end-to-end LSTM-based attention model that recognizes 2-digit numbers.  \n",
    "\n",
    "### Things to do:\n",
    "-----------\n",
    "- Run through the steps and make sure things are making sense.\n",
    "- Try implementing a different type of attention model, and/or tuning hyperparamters (see config.json):\n",
    "  - hidden state size\n",
    "  - num layers\n",
    "  - learning rate\n",
    "  - batch size\n",
    "  - dropout (applied between LSTM layers)\n",
    "  - sampling prob (for scheduled sampling) \n",
    "  - or any other aspect you find interesting\n",
    "- Note: We didn't cover scheduled sampling in lecture (though perhaps previous lectures have covered it) but it\n",
    "refers to decoding during training time. At each time step, a new prediction is made (in this case one of 0-9).  With scheduled sampling, we feed this previous prediction as the input to the next step of the\n",
    "decoder with probability sample_prob; otherwise the ground truth is used. If the sample_prob is set to\n",
    "0, then the ground truth is always fed as input at each timestep.\n",
    "\n",
    "- To get a better understanding of the model's behavior, try plotting some things.  For example:\n",
    "  - The loss and word error rate (WER) vs. time on train and dev.\n",
    "  - For several models trained with a range of scheduled sampling rates, plot sampling rate vs. loss and WER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6_DWGTDWotD"
   },
   "outputs": [],
   "source": [
    "# Some setup\n",
    "from rnn.loader import make_loader, Preprocessor\n",
    "from rnn.model import Seq2Seq\n",
    "from rnn.model import LinearND #Hint: this is useful when defining the modified attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zyv2XTBfWotH"
   },
   "outputs": [],
   "source": [
    "# Define the attention component of the model (included here to make it easy to modify)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim, attn_dim=None):\n",
    "        \"\"\"\n",
    "        Initialize Attention.\n",
    "        ----\n",
    "        enc_dim: encoder hidden state dimension\n",
    "        dec_dim: decoder hidden state dimension\n",
    "        attn_dim: attention feature dimension\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        if enc_dim == dec_dim and attn_dim is None:\n",
    "            self.use_default = True\n",
    "        elif attn_dim is not None:\n",
    "            self.use_default = False\n",
    "            self.attn_dim = attn_dim\n",
    "            self.enc_dim = enc_dim\n",
    "            self.dec_dim = dec_dim\n",
    "            self.v = LinearND(self.attn_dim, 1, bias=False)\n",
    "            self.W1 = LinearND(self.enc_dim, self.attn_dim, bias=False)\n",
    "            self.W2 = nn.Linear(self.dec_dim, self.attn_dim, bias=False)\n",
    "        else:\n",
    "            raise ValueError(\"invalid args (enc_dim={}, dec_dim={}, attn_dim={})\".format(enc_dim, dec_dim, attn_dim))\n",
    "\n",
    "    def forward(self, eh, dhx, ax=None):\n",
    "        \"\"\"\n",
    "        Forward Attention method.\n",
    "        ----\n",
    "        eh (FloatTensor): the encoder hidden state with\n",
    "            shape (batch size, time, hidden dimension).\n",
    "        dhx (FloatTensor): one time step of the decoder hidden\n",
    "            state with shape (batch size, hidden dimension).\n",
    "        ax (FloatTensor): one time step of the attention vector.\n",
    "        ----\n",
    "        Returns the context vectors (sx) and the corresponding attention alignment (ax)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute inner product of decoder slice with every encoder slice\n",
    "        pax = torch.sum(eh * dhx, dim=2)\n",
    "        ax = nn.functional.softmax(pax, dim=1)\n",
    "        sx = torch.sum(eh * ax.unsqueeze(2), dim=1, keepdim=True)\n",
    "\n",
    "        return sx, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIujXwnsWotK"
   },
   "outputs": [],
   "source": [
    "# Training and testing functions\n",
    "\n",
    "def compute_wer(results):\n",
    "    \"\"\"\n",
    "    Compute the word-error-rate (WER).\n",
    "    \"\"\"\n",
    "    dist = 0.\n",
    "    for label, pred in results:\n",
    "        dist += editdistance.eval(label, pred)\n",
    "    total = sum(len(label) for label, _ in results)\n",
    "    return dist / total\n",
    "\n",
    "def train(model, optimizer, ldr):\n",
    "    \"\"\"\n",
    "    Train the model for an epoch (one pass over the training data)\n",
    "    ----\n",
    "    model: Seq2Seq model instance\n",
    "    optimizer: torch.nn optimizer instance\n",
    "    ldr: data loader instance\n",
    "    ----\n",
    "    Returns the average loss over an epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.scheduled_sampling = model.sample_prob != 0\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for ii, (inputs, labels) in enumerate(ldr):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = model.collate(inputs, labels)\n",
    "        loss = model.loss(x, y)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.item())\n",
    "        \n",
    "    return np.mean(losses)\n",
    "\n",
    "def evaluate(model, ldr, preproc):\n",
    "    \"\"\"\n",
    "    Evaluate the model (on either dev or test).\n",
    "    ----\n",
    "    model: Seq2Seq model instance\n",
    "    ldr: data loader instance\n",
    "    preproc: preprocessor instance\n",
    "    ----\n",
    "    Returns the average loss and wer on a given dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.scheduled_sampling = False\n",
    "    \n",
    "    losses, hyps, refs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in ldr:\n",
    "            x, y = model.collate(inputs, labels)\n",
    "            # get loss\n",
    "            loss = model.loss(x, y)\n",
    "            losses.append(loss.data.item())\n",
    "            # get predictions\n",
    "            pred = model.infer(x, y)\n",
    "            hyps.extend(pred)\n",
    "            refs.extend(labels)\n",
    "\n",
    "    results = [(preproc.decode(r), preproc.decode(h)) for r, h in zip(refs, hyps)]\n",
    "    \n",
    "    return np.mean(losses), compute_wer(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "fWx0i-W3WotO",
    "outputId": "7845ad98-5c3e-46e4-d45e-7dc4556fa03e"
   },
   "outputs": [],
   "source": [
    "# Train and test the attention-based recognizer.  With the default config,\n",
    "# you should get <10% word error rate within 15 epochs.\n",
    "\n",
    "import simplejson as json\n",
    "import random\n",
    "\n",
    "RNN_CONFIG_PATH = \"/content/drive/My Drive/MLSS2019/tutorials/rnn/config.json\" if IS_COLAB else \"rnn/config_local.json\"\n",
    "with open(RNN_CONFIG_PATH, \"r\") as fid:                                                                                                                                                                                                                                      \n",
    "    config = json.load(fid)\n",
    "\n",
    "random.seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"Training RNN\")\n",
    "data_cfg = config[\"data\"]\n",
    "model_cfg = config[\"model\"]\n",
    "opt_cfg = config[\"optimizer\"]\n",
    "\n",
    "preproc = Preprocessor(data_cfg[\"train_set\"], start_and_end=data_cfg[\"start_and_end\"])\n",
    "\n",
    "train_ldr = make_loader(data_cfg[\"train_set\"], preproc, opt_cfg[\"batch_size\"])\n",
    "dev_ldr = make_loader(data_cfg[\"dev_set\"], preproc, opt_cfg[\"batch_size\"])\n",
    "\n",
    "attention = Attention(model_cfg[\"encoder\"][\"hidden_size\"], model_cfg[\"decoder\"][\"hidden_size\"])\n",
    "model = Seq2Seq(preproc.input_dim, preproc.vocab_size, attention, model_cfg)\n",
    "model = model.cuda() if use_cuda else model.cpu()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=opt_cfg[\"learning_rate\"], momentum=opt_cfg[\"momentum\"])\n",
    "\n",
    "log=\"epoch {:4} | train_loss={:6.2f}, dev_loss={:6.2f} with {:6.2f}% WER ({:6.2f}s elapsed)\"\n",
    "\n",
    "best_so_far = float(\"inf\")\n",
    "for ep in range(opt_cfg[\"max_epochs\"]):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss = train(model, optimizer, train_ldr)    \n",
    "    dev_loss, dev_wer = evaluate(model, dev_ldr, preproc)\n",
    "    \n",
    "    print(log.format(ep + 1, train_loss, dev_loss, dev_wer * 100., time.time() - start))\n",
    "    \n",
    "    torch.save(model, os.path.join(config[\"save_path\"], str(ep)))\n",
    "    \n",
    "    if dev_wer < best_so_far:\n",
    "        best_so_far = dev_wer\n",
    "        torch.save(model, os.path.join(config[\"save_path\"], \"best\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "DKgvqQR7WotW",
    "outputId": "eb9e3041-c1bd-4c29-d6e9-30abb6cbe506"
   },
   "outputs": [],
   "source": [
    "print(\"Testing RNN\")\n",
    "test_model = torch.load(os.path.join(config[\"save_path\"], \"best\"))\n",
    "test_ldr = make_loader(data_cfg[\"test_set\"], preproc, opt_cfg[\"batch_size\"])\n",
    "\n",
    "_, test_wer = evaluate(test_model, test_ldr, preproc)\n",
    "\n",
    "print(\"{:.2f}% WER (test)\".format(test_wer * 100.))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw4.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
