{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "hw4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS9Jzok0Wosl",
        "colab_type": "code",
        "outputId": "c41ff1ad-5a0c-4cf8-fe3d-81942ba71158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import editdistance\n",
        "import os\n",
        "np.seterr(divide='ignore') # masks log(0) errors"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnItcUgxt7Ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q soundfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJJtp0AXkgub",
        "colab_type": "code",
        "outputId": "09e52491-c552-4f13-aa73-2f868495fdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yoebj7RWosv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hmm_path='/content/drive/My Drive/MLSS2019/tutorials/'\n",
        "import sys\n",
        "sys.path.append(hmm_path)\n",
        "from hmm.multiple import FullGaussianHMM\n",
        "from hmm.single import GaussianHMM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWJ6dgkyWos2",
        "colab_type": "code",
        "outputId": "155bcb40-d140-495c-d1a4-26baa53f64a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "\"\"\"\n",
        "Single Digit HMM\n",
        "\"\"\"\n",
        "data_single_digit = np.load(\"/content/drive/My Drive/MLSS2019/tutorials/hmm/data/mfccs_single.npz\",allow_pickle=True)\n",
        "\n",
        "n_states = 15\n",
        "n_dims = 13\n",
        "n_iter = 0\n",
        "model = dict()\n",
        "digits = range(10)\n",
        "    \n",
        "for digit in digits:\n",
        "    print(\"Training HMM for digit %d\" % digit)\n",
        "    Xtrain_digit = [x for x, y in zip(data_single_digit[\"Xtrain\"], data_single_digit[\"Ytrain\"]) if y == digit]\n",
        "    model[digit] = GaussianHMM(n_states=n_states, n_dims=n_dims)\n",
        "    model[digit].init_gaussian_params(Xtrain_digit)\n",
        "    model[digit].init_hmm_params()\n",
        "    for i in range(n_iter):\n",
        "        print(\"starting iteration {}...\".format(i+1))\n",
        "        model[digit].train(Xtrain_digit)\n",
        "\n",
        "print(\"Testing HMM\")\n",
        "accuracy = np.zeros(10)\n",
        "confusion = np.zeros((10, 10))\n",
        "for x, y in zip(data_single_digit[\"Xtest\"], data_single_digit[\"Ytest\"]):\n",
        "    T = len(x)\n",
        "\n",
        "    scores = []\n",
        "    for digit in digits:\n",
        "        log_pi = np.log(model[digit].pi)\n",
        "        log_A = np.log(model[digit].A)\n",
        "        log_B = model[digit].get_emissions(x)\n",
        "        _, log_prob = model[digit].viterbi(log_pi, log_A, log_B)\n",
        "        scores.append(log_prob)\n",
        "\n",
        "    top_digit, top_log_prob = sorted(zip(digits, scores), key=lambda x: -x[1])[0]\n",
        "    confusion[y, top_digit] += 1.\n",
        "\n",
        "accuracy = np.diag(confusion) / confusion.sum(axis=1)\n",
        "\n",
        "print(\"accuracy ({:.4f}): {}\".format(accuracy.mean(), accuracy))\n",
        "\n",
        "with open(\"/content/drive/My Drive/MLSS2019/tutorials/hmm/single_digit_model.pkl\", \"wb\") as f:\n",
        "    pkl.dump(model, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training HMM for digit 0\n",
            "Training HMM for digit 1\n",
            "Training HMM for digit 2\n",
            "Training HMM for digit 3\n",
            "Training HMM for digit 4\n",
            "Training HMM for digit 5\n",
            "Training HMM for digit 6\n",
            "Training HMM for digit 7\n",
            "Training HMM for digit 8\n",
            "Training HMM for digit 9\n",
            "Testing HMM\n",
            "accuracy (0.9833): [1.         0.95833333 0.95833333 1.         0.95833333 1.\n",
            " 1.         1.         0.95833333 1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-llfxRAWos9",
        "colab_type": "code",
        "outputId": "1718fbc8-382b-4c59-f1e9-320978aa46f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\"\"\"\n",
        "Multiple Digit HMM\n",
        "\"\"\"\n",
        "data_multiple_digit = np.load(\"/content/drive/My Drive/MLSS2019/tutorials/hmm/data/mfccs_multiple.npz\",allow_pickle=True)\n",
        "full_model = FullGaussianHMM(data_multiple_digit[\"Xtrain\"], \"/content/drive/My Drive/MLSS2019/tutorials/hmm/single_digit_model.pkl\")\n",
        "\n",
        "n_iter = 0\n",
        "\n",
        "print(\"Training HMM\")\n",
        "for i in range(n_iter):\n",
        "    print(\"starting iteration {}...\".format(i + 1))\n",
        "    full_model.train(data_multiple_digit[\"Xtrain\"], data_multiple_digit[\"Ytrain\"])\n",
        "\n",
        "print(\"Testing HMM\")\n",
        "test_wer = full_model.test(data_multiple_digit[\"Xtest\"], data_multiple_digit[\"Ytest\"])\n",
        "print(\"{:.2f}% WER\".format(test_wer * 100.))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training HMM\n",
            "Testing HMM\n",
            "56.56% WER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6_DWGTDWotD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_path='/content/drive/My Drive/MLSS2019/tutorials/'\n",
        "sys.path.append(rnn_path)\n",
        "\n",
        "from hmm.multiple import FullGaussianHMM\n",
        "from hmm.single import GaussianHMM\n",
        "\n",
        "from rnn.loader import make_loader, Preprocessor\n",
        "from rnn.model import Seq2Seq\n",
        "from rnn.model import LinearND #Hint: this is useful when defining the modified attention mechanism"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyv2XTBfWotH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_dim, dec_dim, attn_dim=None):\n",
        "        \"\"\"\n",
        "        Initialize Attention.\n",
        "        ----\n",
        "        enc_dim: encoder hidden state dimension\n",
        "        dec_dim: decoder hidden state dimension\n",
        "        attn_dim: attention feature dimension\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        if enc_dim == dec_dim and attn_dim is None:\n",
        "            self.use_default = True\n",
        "        elif attn_dim is not None:\n",
        "            self.use_default = False\n",
        "            self.attn_dim = attn_dim\n",
        "            self.enc_dim = enc_dim\n",
        "            self.dec_dim = dec_dim\n",
        "            self.v = LinearND(self.attn_dim, 1, bias=False)\n",
        "            self.W1 = LinearND(self.enc_dim, self.attn_dim, bias=False)\n",
        "            self.W2 = nn.Linear(self.dec_dim, self.attn_dim, bias=False)\n",
        "        else:\n",
        "            raise ValueError(\"invalid args (enc_dim={}, dec_dim={}, attn_dim={})\".format(enc_dim, dec_dim, attn_dim))\n",
        "\n",
        "    def forward(self, eh, dhx, ax=None):\n",
        "        \"\"\"\n",
        "        Forward Attention method.\n",
        "        ----\n",
        "        eh (FloatTensor): the encoder hidden state with\n",
        "            shape (batch size, time, hidden dimension).\n",
        "        dhx (FloatTensor): one time step of the decoder hidden\n",
        "            state with shape (batch size, hidden dimension).\n",
        "        ax (FloatTensor): one time step of the attention vector.\n",
        "        ----\n",
        "        Returns the context vectors (sx) and the corresponding attention alignment (ax)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Compute inner product of decoder slice with every encoder slice\n",
        "        pax = torch.sum(eh * dhx, dim=2)\n",
        "        ax = nn.functional.softmax(pax, dim=1)\n",
        "        sx = torch.sum(eh * ax.unsqueeze(2), dim=1, keepdim=True)\n",
        "\n",
        "        return sx, ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIujXwnsWotK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_wer(results):\n",
        "    \"\"\"\n",
        "    Compute the word-error-rate (WER).\n",
        "    \"\"\"\n",
        "    dist = 0.\n",
        "    for label, pred in results:\n",
        "        dist += editdistance.eval(label, pred)\n",
        "    total = sum(len(label) for label, _ in results)\n",
        "    return dist / total\n",
        "\n",
        "def train(model, optimizer, ldr):\n",
        "    \"\"\"\n",
        "    Train the model for an epoch (one pass over the training data)\n",
        "    ----\n",
        "    model: Seq2Seq model instance\n",
        "    optimizer: torch.nn optimizer instance\n",
        "    ldr: data loader instance\n",
        "    ----\n",
        "    Returns the average loss over an epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.scheduled_sampling = model.sample_prob != 0\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    for ii, (inputs, labels) in enumerate(ldr):\n",
        "        optimizer.zero_grad()\n",
        "        x, y = model.collate(inputs, labels)\n",
        "        loss = model.loss(x, y)\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data.item())\n",
        "        \n",
        "    return np.mean(losses)\n",
        "\n",
        "def evaluate(model, ldr, preproc):\n",
        "    \"\"\"\n",
        "    Evaluate the model (on either dev or test).\n",
        "    ----\n",
        "    model: Seq2Seq model instance\n",
        "    ldr: data loader instance\n",
        "    preproc: preprocessor instance\n",
        "    ----\n",
        "    Returns the average loss and wer on a given dataset\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.scheduled_sampling = False\n",
        "    \n",
        "    losses, hyps, refs = [], [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in ldr:\n",
        "            x, y = model.collate(inputs, labels)\n",
        "            # get loss\n",
        "            loss = model.loss(x, y)\n",
        "            losses.append(loss.data.item())\n",
        "            # get predictions\n",
        "            pred = model.infer(x, y)\n",
        "            hyps.extend(pred)\n",
        "            refs.extend(labels)\n",
        "\n",
        "    results = [(preproc.decode(r), preproc.decode(h)) for r, h in zip(refs, hyps)]\n",
        "    \n",
        "    return np.mean(losses), compute_wer(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWx0i-W3WotO",
        "colab_type": "code",
        "outputId": "9ce4d95a-faa2-4833-fdee-c8f12f406e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "!pip install -q simplejson\n",
        "import simplejson as json\n",
        "import random\n",
        "\"\"\"\n",
        "Use the development set to tune your model.\n",
        "------\n",
        "With the default config, can get <10% dev WER within 15 epochs.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/drive/My Drive/MLSS2019/tutorials/rnn/config.json\", \"r\") as fid:                                                                                                                                                                                                                                      \n",
        "    config = json.load(fid)\n",
        "\n",
        "random.seed(config[\"seed\"])\n",
        "np.random.seed(config[\"seed\"])\n",
        "torch.manual_seed(config[\"seed\"])\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"Training RNN\")\n",
        "data_cfg = config[\"data\"]\n",
        "model_cfg = config[\"model\"]\n",
        "opt_cfg = config[\"optimizer\"]\n",
        "\n",
        "preproc = Preprocessor(data_cfg[\"train_set\"], start_and_end=data_cfg[\"start_and_end\"])\n",
        "\n",
        "train_ldr = make_loader(data_cfg[\"train_set\"], preproc, opt_cfg[\"batch_size\"])\n",
        "dev_ldr = make_loader(data_cfg[\"dev_set\"], preproc, opt_cfg[\"batch_size\"])\n",
        "\n",
        "attention = Attention(model_cfg[\"encoder\"][\"hidden_size\"], model_cfg[\"decoder\"][\"hidden_size\"])\n",
        "model = Seq2Seq(preproc.input_dim, preproc.vocab_size, attention, model_cfg)\n",
        "model = model.cuda() if use_cuda else model.cpu()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=opt_cfg[\"learning_rate\"], momentum=opt_cfg[\"momentum\"])\n",
        "\n",
        "log=\"epoch {:4} | train_loss={:6.2f}, dev_loss={:6.2f} with {:6.2f}% WER ({:6.2f}s elapsed)\"\n",
        "\n",
        "best_so_far = float(\"inf\")\n",
        "for ep in range(opt_cfg[\"max_epochs\"]):\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss = train(model, optimizer, train_ldr)    \n",
        "    dev_loss, dev_wer = evaluate(model, dev_ldr, preproc)\n",
        "    \n",
        "    print(log.format(ep + 1, train_loss, dev_loss, dev_wer * 100., time.time() - start))\n",
        "    \n",
        "    torch.save(model, os.path.join(config[\"save_path\"], str(ep)))\n",
        "    \n",
        "    if dev_wer < best_so_far:\n",
        "        best_so_far = dev_wer\n",
        "        torch.save(model, os.path.join(config[\"save_path\"], \"best\"))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training RNN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch    1 | train_loss=  5.38, dev_loss=  4.56 with  80.71% WER (  9.78s elapsed)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch    2 | train_loss=  4.05, dev_loss=  3.81 with  64.82% WER (  9.83s elapsed)\n",
            "epoch    3 | train_loss=  3.53, dev_loss=  3.39 with  59.82% WER (  9.94s elapsed)\n",
            "epoch    4 | train_loss=  3.11, dev_loss=  2.89 with  49.29% WER (  9.93s elapsed)\n",
            "epoch    5 | train_loss=  2.63, dev_loss=  3.77 with  60.00% WER ( 10.16s elapsed)\n",
            "epoch    6 | train_loss=  2.48, dev_loss=  2.51 with  41.07% WER ( 10.09s elapsed)\n",
            "epoch    7 | train_loss=  2.08, dev_loss=  2.22 with  32.32% WER (  9.94s elapsed)\n",
            "epoch    8 | train_loss=  1.80, dev_loss=  1.07 with  16.79% WER ( 10.01s elapsed)\n",
            "epoch    9 | train_loss=  0.97, dev_loss=  0.83 with  13.57% WER ( 10.03s elapsed)\n",
            "epoch   10 | train_loss=  0.77, dev_loss=  0.78 with  11.25% WER ( 10.02s elapsed)\n",
            "epoch   11 | train_loss=  0.72, dev_loss=  1.05 with  16.07% WER ( 10.26s elapsed)\n",
            "epoch   12 | train_loss=  0.68, dev_loss=  0.77 with  11.25% WER ( 10.06s elapsed)\n",
            "epoch   13 | train_loss=  0.70, dev_loss=  0.77 with  13.57% WER ( 10.02s elapsed)\n",
            "epoch   14 | train_loss=  0.79, dev_loss=  1.36 with  24.82% WER ( 10.03s elapsed)\n",
            "epoch   15 | train_loss=  0.69, dev_loss=  0.62 with   8.04% WER ( 10.03s elapsed)\n",
            "epoch   16 | train_loss=  0.72, dev_loss=  0.93 with  12.86% WER ( 10.02s elapsed)\n",
            "epoch   17 | train_loss=  0.81, dev_loss=  0.69 with  10.36% WER ( 10.03s elapsed)\n",
            "epoch   18 | train_loss=  0.87, dev_loss=  1.14 with  17.32% WER ( 10.29s elapsed)\n",
            "epoch   19 | train_loss=  1.06, dev_loss=  0.90 with  13.39% WER ( 10.13s elapsed)\n",
            "epoch   20 | train_loss=  0.70, dev_loss=  1.65 with  24.29% WER ( 10.21s elapsed)\n",
            "epoch   21 | train_loss=  0.48, dev_loss=  0.68 with  10.71% WER ( 10.06s elapsed)\n",
            "epoch   22 | train_loss=  0.44, dev_loss=  0.80 with  14.29% WER ( 10.11s elapsed)\n",
            "epoch   23 | train_loss=  0.40, dev_loss=  0.56 with   8.57% WER ( 10.02s elapsed)\n",
            "epoch   24 | train_loss=  0.35, dev_loss=  0.58 with   9.64% WER ( 10.20s elapsed)\n",
            "epoch   25 | train_loss=  0.38, dev_loss=  0.70 with  10.54% WER ( 10.01s elapsed)\n",
            "epoch   26 | train_loss=  0.30, dev_loss=  0.56 with   8.21% WER ( 10.06s elapsed)\n",
            "epoch   27 | train_loss=  0.73, dev_loss=  1.84 with  31.43% WER ( 10.07s elapsed)\n",
            "epoch   28 | train_loss=  0.58, dev_loss=  0.79 with  11.79% WER ( 10.08s elapsed)\n",
            "epoch   29 | train_loss=  0.55, dev_loss=  0.54 with   7.86% WER ( 10.04s elapsed)\n",
            "epoch   30 | train_loss=  0.32, dev_loss=  0.42 with   6.61% WER ( 10.25s elapsed)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpcUdhVUWotS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: tune on the dev set\n",
        "# may want to set up function or chunk of code here to perform tuning\n",
        "# call train on training set, call evaluate on dev, save/plot/compare results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKgvqQR7WotW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "eb9e3041-c1bd-4c29-d6e9-30abb6cbe506"
      },
      "source": [
        "print(\"Testing RNN\")\n",
        "test_model = torch.load(os.path.join(config[\"save_path\"], \"best\"))\n",
        "test_ldr = make_loader(data_cfg[\"test_set\"], preproc, opt_cfg[\"batch_size\"])\n",
        "\n",
        "_, test_wer = evaluate(test_model, test_ldr, preproc)\n",
        "\n",
        "print(\"{:.2f}% WER (test)\".format(test_wer * 100.))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing RNN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6.19% WER (test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4j66kEcWotc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}