{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Summer School - London 2019\n",
    "## Reinforcement Learning Tutorial\n",
    "\n",
    "### Author: [Katja Hofmann](https://www.microsoft.com/en-us/research/people/kahofman/)\n",
    "\n",
    "This tutorial uses the [MineRL package](http://minerl.io/) to illustrate how a Reinforcement Learning (RL) agent can learn to interact with the popular video game [Minecraft](https://www.minecraft.net/en-us/). MineRL was developed by a team led by [William H. Guss](http://wguss.ml/) and [Brandon Houghton](https://github.com/brandonhoughton) for the NeurIPS 2019 MineRL competition, hosted by AICrowd and sponsored by Microsoft. MineRL is based on [Project Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/), developed at [Microsoft Research](https://www.microsoft.com/en-us/research/theme/game-intelligence/). This tutorial uses the deep learning framework [chainer](https://chainer.org/) to implement RL algorithms.\n",
    "\n",
    "**Further reading:**\n",
    "- [MineRL Competition at AICrowd](https://www.aicrowd.com/challenges/neurips-2019-minerl-competition)\n",
    "- [Guss et al. 2019: The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors](https://arxiv.org/abs/1904.10079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "If this is the first time you run the tutorial, install the latest MineRL package as shown below. \n",
    "For details and prerequisites, see http://minerl.io/docs/tutorials/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment to install minerl\n",
    "# check recommended version - currently: 0.1.9\n",
    "#!pip install --upgrade minerl==0.1.9\n",
    "#!pip install --upgrade chainer, cv2, matplotlib, pylab, logging, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environments\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "# chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import initializers\n",
    "from chainer import serializers\n",
    "from chainer import optimizers, Chain, Variable\n",
    "\n",
    "# visualization\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import pylab\n",
    "from IPython import display\n",
    "\n",
    "# get DEBUG logging from MineRL while Minecraft starts up\n",
    "import sys\n",
    "import logging\n",
    "logger = logging.getLogger(\"minerl\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# utilities\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Test your MineRL installation by instantiating an environment, and creating a first agent to navigate this environment. The example below is based on the [MineRL Tutorial](http://minerl.io/docs/tutorials/first_agent.html).\n",
    "\n",
    "First, we instantiate a MineRL gym environment. This will take a couple of minutes, as Minecraft is started in the background. Debug output will be generated while Minecraft is starting. The generated output can usually be ignored, but can be useful in case something goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a MineRL environment - be patient\n",
    "nav_env = gym.make('MineRLNavigateDense-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready for our first interaction with the environment. Initially, actions are hard coded to move towards the direction indicated by the compass, as shown in the [MineRL Tutorial](http://minerl.io/docs/tutorials/first_agent.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test interaction with the environment - setup\n",
    "\n",
    "obs, _ = nav_env.reset() # this may take up to a minute\n",
    "done = False\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "net_reward = 0\n",
    "stepcount = 0\n",
    "maxsteps = 1000\n",
    "\n",
    "# prepare visuals\n",
    "fig = pylab.figure(figsize=(10, 5))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "ax1 = pylab.subplot(gs[0, 0])\n",
    "ax1.xaxis.set_visible(False)\n",
    "ax1.yaxis.set_visible(False)\n",
    "imgplot = ax1.imshow(obs['pov'])\n",
    "ax2 = pylab.subplot(gs[0, 1])\n",
    "rewards = [0]\n",
    "line, = ax2.plot(range(len(rewards)), rewards)\n",
    "pylab.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact with the environment, results are updated in the plot above\n",
    "while not done:\n",
    "    action = nav_env.action_space.noop()\n",
    "    action['camera'] = [0, 0.03*obs['compassAngle']]\n",
    "    action['back'] = 0\n",
    "    action['forward'] = 1\n",
    "    action['jump'] = 1\n",
    "    action['attack'] = 1\n",
    "\n",
    "    obs, reward, done, info = nav_env.step(action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    if stepcount % 50 == 0:\n",
    "        imgplot.set_data(obs['pov'])\n",
    "        line.set_data(range(len(rewards)), rewards)\n",
    "        ax2.set_xlim(0, len(rewards))\n",
    "        ax2.set_ylim(min(rewards), max(rewards))\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    stepcount += 1\n",
    "    if stepcount >= maxsteps:\n",
    "        break\n",
    "\n",
    "print(\"Time taken for %d steps: %.1f seconds\" % (maxsteps, time() - start))\n",
    "print(\"Total reward: \", sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the last action taken\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Experiment Setup\n",
    "\n",
    "In this section, we set up a number of components that make it easy to run and visualize RL experiments in Jupyter Notebooks. The main abstractions are:\n",
    "- Environment: defines an interactive control task. We assume environments implement the [OpenAI gym interface](https://gym.openai.com/). In addition to the MineRL environment introduced above, we will also implement a toy task, SimpleRooms, to illustrate typical environment functionality.\n",
    "- Agent: interacts with an environment by receiving observations and rewards, and taking actions. We will implement several agents throughout this tutorial, starting from a random agent and moving to a Deep Q-Network agent that implements Q-Learning.\n",
    "- Experiment: connects agents and environments, collect and report results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment interface\n",
    "\n",
    "class Environment(object):\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError('Inheriting classes must override reset.')\n",
    "\n",
    "    def actions(self):\n",
    "        raise NotImplementedError('Inheriting classes must override actions.')\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError('Inheriting classes must override step')\n",
    "\n",
    "class ActionSpace(object):\n",
    "    \n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.n = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRooms(Environment):\n",
    "    \"\"\"Define a simple 4-room environment with 16 states\n",
    "       actions: 0 - north, 1 - east, 2 - west, 3 - south\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleRooms, self).__init__()\n",
    "\n",
    "        # define state and action space\n",
    "        self.S = range(16)\n",
    "        self.action_space = ActionSpace(range(4))\n",
    "\n",
    "        # define reward structure\n",
    "        self.R = [0] * len(self.S)\n",
    "        self.R[random.choice(self.S)] = 1\n",
    "\n",
    "        # define transitions\n",
    "        self.P = {}\n",
    "        self.P[0] = [1, 4]\n",
    "        self.P[1] = [0, 2, 5]\n",
    "        self.P[2] = [1, 3, 6]\n",
    "        self.P[3] = [2, 7]\n",
    "        self.P[4] = [0, 5, 8]\n",
    "        self.P[5] = [1, 4]\n",
    "        self.P[6] = [2, 7]\n",
    "        self.P[7] = [3, 6, 11]\n",
    "        self.P[8] = [4, 9, 12]\n",
    "        self.P[9] = [8, 13]\n",
    "        self.P[10] = [11, 14]\n",
    "        self.P[11] = [7, 10, 15]\n",
    "        self.P[12] = [8, 13]\n",
    "        self.P[13] = [9, 12, 14]\n",
    "        self.P[14] = [10, 13, 15]\n",
    "        self.P[15] = [11, 14]\n",
    "\n",
    "        self.max_trajectory_length = 50\n",
    "        self.tolerance = 0.1\n",
    "        self._rendered_maze = self._render_maze()\n",
    "\n",
    "    def step(self, action):\n",
    "        s_prev = self.s\n",
    "        self.s = self.single_step(self.s, action)\n",
    "        reward = self.single_reward(self.s, s_prev, self.R)\n",
    "        self.nstep += 1\n",
    "        self.is_reset = False\n",
    "\n",
    "        if (reward < -1. * (self.tolerance) or reward > self.tolerance) or self.nstep == self.max_trajectory_length:\n",
    "            self.reset()\n",
    "\n",
    "        return (self._convert_state(self.s), reward, self.is_reset, '')\n",
    "\n",
    "    def single_step(self, s, a):\n",
    "        if a < 0 or a > 3:\n",
    "            raise ValueError('Unknown action', a)\n",
    "        if a == 0 and (s-4 in self.P[s]):\n",
    "            s -= 4\n",
    "        elif a == 1 and (s+1 in self.P[s]):\n",
    "            s += 1\n",
    "        elif a == 2 and (s-1 in self.P[s]):\n",
    "            s -= 1\n",
    "        elif a == 3 and (s+4 in self.P[s]):\n",
    "            s += 4\n",
    "        return s\n",
    "\n",
    "    def single_reward(self, s, s_prev, rewards):\n",
    "        if s == s_prev:\n",
    "            return 0\n",
    "        return rewards[s]\n",
    "\n",
    "    def reset(self):\n",
    "        self.nstep = 0\n",
    "        self.s = random.choice(self.S)\n",
    "        # disallow spawning in a reward state\n",
    "        while (self.R[self.s] < -1. * (self.tolerance) or self.R[self.s] > self.tolerance):\n",
    "            self.s = random.choice(self.S)\n",
    "        self.is_reset = True\n",
    "        return self._convert_state(self.s)\n",
    "\n",
    "    def _convert_state(self, s):\n",
    "        converted = np.zeros(len(self.S), dtype=np.float32)\n",
    "        converted[s] = 1\n",
    "        return converted\n",
    "\n",
    "    def _get_render_coords(self, s):\n",
    "        return (int(s / 4) * 4, (s % 4) * 4)\n",
    "\n",
    "    def _render_maze(self):\n",
    "        # draw background and grid lines\n",
    "        maze = np.zeros((17, 17))\n",
    "        for x in range(0, 17, 4):\n",
    "            maze[x, :] = 0.5\n",
    "        for y in range(0, 17, 4):\n",
    "            maze[:, y] = 0.5\n",
    "\n",
    "        # draw reward and transitions\n",
    "        for s in range(16):\n",
    "            if self.R[s] != 0:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x+1:x+4, y+1:y+4] = self.R[s]\n",
    "            if self.single_step(s, 0) == s:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x, y:y+5] = -1\n",
    "            if self.single_step(s, 1) == s:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x:x+5, y+4] = -1\n",
    "            if self.single_step(s, 2) == s:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x:x+5, y] = -1\n",
    "            if self.single_step(s, 3) == s:\n",
    "                x, y = self._get_render_coords(s)\n",
    "                maze[x+4, y:y+4] = -1\n",
    "        return maze\n",
    "\n",
    "    def render(self, mode = 'rgb_array'):\n",
    "        assert mode == 'rgb_array', 'Unknown mode: %s' % mode\n",
    "        img = np.array(self._rendered_maze, copy=True)\n",
    "\n",
    "        # draw current agent location\n",
    "        x, y = self._get_render_coords(self.s)\n",
    "        img[x+1:x+4, y+1:y+4] = 2.0\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    '''Agent base class'''\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.num_actions = len(actions)\n",
    "\n",
    "    def step(self, obs, reward, done, info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    '''Agent that samples actions uniformly at random'''\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        super(RandomAgent, self).__init__(actions)\n",
    "    \n",
    "    def step(self, obs, reward, done, info):\n",
    "        self.current_loss = random.random()\n",
    "        return random.randint(0, self.num_actions-1)\n",
    "\n",
    "class DefaultAgent(Agent):\n",
    "    '''Agent that always takes a default action'''\n",
    "    def __init__(self, actions, default_action):\n",
    "        super(DefaultAgent, self).__init__(actions)\n",
    "        self.default_action = default_action\n",
    "    \n",
    "    def step(self, obs, reward, done, info):\n",
    "        self.current_loss = random.random()\n",
    "        return self.default_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "\n",
    "    def __init__(self, env, agent, normobs=False):\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "\n",
    "        self.epoch_losses = [0]\n",
    "        self.rolling_average = np.array([0])\n",
    "        self.windowsize = 100\n",
    "        self.normalize_observations = normobs\n",
    "\n",
    "        # prepare visuals\n",
    "        self.fig = pylab.figure(figsize=(10, 5))\n",
    "        gs = gridspec.GridSpec(2, 2)\n",
    "        self.ax = pylab.subplot(gs[:, 0])\n",
    "        self.ax.title.set_text('Current frame')\n",
    "        self.ax.xaxis.set_visible(False)\n",
    "        self.ax.yaxis.set_visible(False)\n",
    "        self.ax1 = pylab.subplot(gs[0, 1])\n",
    "        self.ax1.title.set_text('Rolling average reward')\n",
    "        self.ax2 = pylab.subplot(gs[1, 1])\n",
    "        self.ax2.title.set_text('Average loss')\n",
    "        \n",
    "        self.line, = self.ax1.plot(range(len(self.rolling_average)), self.rolling_average)\n",
    "        self.line2, = self.ax2.plot(range(len(self.epoch_losses)), self.epoch_losses)\n",
    "        self.imgplot = self.ax.imshow(np.random.random((64,64)), interpolation='none', cmap='viridis')\n",
    "        self.first_render = True\n",
    "\n",
    "        pylab.show()\n",
    "\n",
    "    def run(self, num_steps, display_frequency):\n",
    "        self.display_frequency = display_frequency\n",
    "        observation = self.env.reset()\n",
    "        self.update_display()\n",
    "        steps = 0\n",
    "        done = False\n",
    "        reward = .0\n",
    "        rewards = np.array([])\n",
    "        losses = []\n",
    "\n",
    "        while steps < num_steps:\n",
    "            steps += 1\n",
    "            if self.normalize_observations:\n",
    "                observation = (observation / 255.).astype(np.float32, copy=False)\n",
    "            action = self.agent.step(observation, reward, done, None)\n",
    "            observation, reward, done, _ = self.env.step(action)\n",
    "            losses.append(self.agent.current_loss)\n",
    "\n",
    "            if done:\n",
    "                observation = self.env.reset()\n",
    "\n",
    "            rewards = np.append(rewards, reward)\n",
    "            self.rolling_average = np.append(self.rolling_average,\n",
    "                                        sum(rewards[-self.windowsize:])/len(rewards[-self.windowsize:]))\n",
    "\n",
    "            if steps % self.display_frequency == 0:\n",
    "                self.epoch_losses = np.append(self.epoch_losses, np.mean(losses))\n",
    "                self.update_display()\n",
    "                losses = []\n",
    "      \n",
    "    def update_display(self):\n",
    "        self.imgplot.set_data(self.env.render(mode='rgb_array'))\n",
    "        self.line.set_data(range(len(self.rolling_average)), self.rolling_average)\n",
    "        self.ax1.set_xlim(0, max(100, len(self.rolling_average)))\n",
    "        self.ax1.set_ylim(min(self.rolling_average)-0.01, max(self.rolling_average)+0.01 * 1.1)\n",
    "\n",
    "        self.line2.set_data(range(0, len(self.epoch_losses)), self.epoch_losses)\n",
    "        self.ax2.set_xlim(0, max(100, len(self.epoch_losses)))\n",
    "        self.ax2.set_ylim(min(min(self.epoch_losses), 1e-5), max(self.epoch_losses)+0.01 * 1.1)\n",
    "        self.fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Random Agent on Simple Rooms\n",
    "\n",
    "We are ready to set up a first simple experiment. This illustrates how an experiment connects environment and agent. We'll run a random agent on the SimpleRooms environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "simple_env = SimpleRooms()\n",
    "random_agent = RandomAgent(simple_env.action_space.actions)\n",
    "experiment = Experiment(simple_env, random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment for 1000 steps\n",
    "experiment.run(1000, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Random Agent on MineRL-NavDense\n",
    "\n",
    "Now for the real thing - our first experiment with the MineRL environment. We will simplify the environment, for illustration and to make learning the task within a couple of minutes feasible. We'll simplify the action and observation space, as well as providing a simpler reward signal, as implemented in the environment wrapper below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMinecraftEnvWrapper(Environment):\n",
    "    '''Wrap a MineRL environment to discretize actions - assume Nav environemnt'''\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        # define action space\n",
    "        self.action_space = ActionSpace(range(3))\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs, _ = self.env.reset()\n",
    "        self.steps_this_episode = 0\n",
    "        return self._convert_obs(self.obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps_this_episode += 1\n",
    "        self.obs, self.reward, self.done, self.info = self.env.step(self._convert_action(action))\n",
    "        # simplify reward signal\n",
    "        if action == 0:\n",
    "            if obs['compassAngle'] < 1:\n",
    "                self.reward = .1\n",
    "            else:\n",
    "                self.reward = .01\n",
    "        else:\n",
    "            self.reward = -.1\n",
    "        return self._convert_obs(self.obs), self.reward, self.done, self.info\n",
    "\n",
    "    def _convert_obs(self, obs):\n",
    "        '''Extract visuals'''\n",
    "        # constructs obs of size 3 x 3 x 3 + 1 = 28\n",
    "        low_res = cv2.resize(obs['pov'], dsize=(3, 3), interpolation=cv2.INTER_NEAREST)\n",
    "        return np.float32(np.hstack([low_res.flatten(), obs['compassAngle']]))\n",
    "\n",
    "    def _convert_action(self, action):\n",
    "        base_action =  self.env.action_space.noop()\n",
    "        base_action['jump'] = 1\n",
    "        base_action['attack'] = 1\n",
    "\n",
    "        if action == 0:\n",
    "            # move forward\n",
    "            base_action['forward'] = 1\n",
    "        elif action == 1:\n",
    "            # turn towards the compass direction\n",
    "            base_action['camera'] = [0, 0.03 * obs['compassAngle']]\n",
    "        elif action == 2:\n",
    "            # move back\n",
    "            base_action['back'] = 1\n",
    "        else:\n",
    "            raise NotImplementedError('Action %d is not implemented.' % action)\n",
    "\n",
    "        return base_action\n",
    "\n",
    "    def render(self, mode):\n",
    "        return self.obs['pov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "wrapped_env = DiscreteMinecraftEnvWrapper(nav_env)\n",
    "random_agent = RandomAgent(wrapped_env.action_space.actions)\n",
    "# default_agent = DefaultAgent(wrapped_env.action_space.actions, 0)\n",
    "experiment = Experiment(wrapped_env, random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment for 500 steps\n",
    "experiment.run(500, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when done using the MineRL experiment, close it down - this will stop the Minecraft client\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL: DQN Agent\n",
    "\n",
    "We are ready to implement our reinforcement learning agent. The code below implements the DQN agent by [Mnih et al. 2015](https://www.nature.com/articles/nature14236/), but instead of a convolutional network we will use 2 fully connected layers (to allow running experiments in reasonable time without GPU).\n",
    "\n",
    "The QLearningAgent class lays out the required components: model network, target network, explorer, replay memory, and optimizer. The components are implemented in turn below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    \"\"\"Q-Learning agent with function approximation.\"\"\"\n",
    "\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        super(QLearningAgent, self).__init__(actions)\n",
    "\n",
    "        self.obs_size = obs_size\n",
    "        self.tau = kwargs.get('tau', .0001)\n",
    "        \n",
    "        self.model_network = QNetwork(self.obs_size, self.num_actions, kwargs.get('nhidden', 512))\n",
    "        self.target_network = QNetwork(self.obs_size, self.num_actions, kwargs.get('nhidden', 512))\n",
    "        self.target_network.copyparams(self.model_network)\n",
    "\n",
    "        self.explorer = EpsilonGreedyExplorer(kwargs.get('epsilon', .1), self.num_actions, self.model_network)\n",
    "\n",
    "        self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 100))\n",
    "        self.optimizer = self.init_optimizer(self.model_network, kwargs.get('learning_rate', .01))\n",
    "\n",
    "        self.gamma = kwargs.get('gamma', .99)\n",
    "        self.minibatch_size = kwargs.get('minibatch_size', 32)\n",
    "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
    "        \n",
    "        self.step_counter = 0\n",
    "        self.current_loss = .0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is the solution version - students should implement the explorer and model update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyExplorer(object):\n",
    "    \"\"\"Implements an epsilon greedy exploration policy\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon, num_actions, model):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = num_actions\n",
    "        self.model = model\n",
    "\n",
    "    def next_action(self, state):\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            # explore\n",
    "            return random.randint(0, self.num_actions-1)\n",
    "\n",
    "        # exploit\n",
    "        Q = self.model(state)\n",
    "        action_index = Q.data.argmax()\n",
    "        return action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, obs, reward, done, info):\n",
    "\n",
    "    if self.step_counter > 0:\n",
    "        self.memory.observe(self.prev_obs, self.prev_action, reward, done)\n",
    "\n",
    "    action = self.explorer.next_action(\n",
    "                Variable(obs.reshape(1, obs.shape[0])))\n",
    "\n",
    "    # start training after 1 epoch\n",
    "    if self.step_counter > self.epoch_length:\n",
    "        self.current_loss = self.update_model()\n",
    "\n",
    "    self.step_counter += 1\n",
    "    self.prev_action = action\n",
    "    self.prev_obs = obs\n",
    "\n",
    "    # decay epsilon after each epoch\n",
    "    if self.step_counter % self.epoch_length == 0:\n",
    "        self.explorer.epsilon = max(0.05, self.explorer.epsilon * .95)\n",
    "\n",
    "    return action\n",
    "\n",
    "QLearningAgent.step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(Chain):\n",
    "    \"\"\"The neural network architecture as a Chainer Chain - here: single hidden layer\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, num_actions, nhidden):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        # use LeCunUniform weight initialization for weights\n",
    "        self.initializer = initializers.LeCunUniform()\n",
    "        self.bias_initializer = initializers.Uniform(1e-4)\n",
    "\n",
    "        super(QNetwork, self).__init__(\n",
    "            feature_layer = L.Linear(obs_size, nhidden,\n",
    "                                initialW = self.initializer,\n",
    "                                initial_bias = self.bias_initializer),\n",
    "            action_values = L.Linear(nhidden, num_actions, \n",
    "                                initialW=self.initializer,\n",
    "                                initial_bias = self.bias_initializer)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"implements forward pass\"\"\"\n",
    "        h = F.relu(self.feature_layer(x))\n",
    "        return self.action_values(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(self):\n",
    "    (s, action, reward, s_next, is_terminal) = self.memory.sample_minibatch(self.minibatch_size)\n",
    "\n",
    "    # compute Q targets (max_a' Q_hat(s_next, a'))\n",
    "    Q_hat = self.target_network(s_next)\n",
    "    Q_hat_max = F.max(Q_hat, axis=1, keepdims=True)\n",
    "    y = (1-is_terminal)*self.gamma*Q_hat_max + reward\n",
    "\n",
    "    # compute Q(s, action)\n",
    "    Q = self.model_network(s)\n",
    "    Q_subset = F.reshape(F.select_item(Q, action), (self.minibatch_size, 1))\n",
    "\n",
    "    # compute Huber loss\n",
    "    error = y - Q_subset\n",
    "    loss_clipped = abs(error) * (abs(error.data) > 1) + (error**2) * (abs(error.data) <= 1)\n",
    "    loss = F.sum(loss_clipped) / self.minibatch_size\n",
    "\n",
    "    # perform model update\n",
    "    self.model_network.zerograds() ## zero out the accumulated gradients in all network parameters\n",
    "    loss.backward()\n",
    "    self.optimizer.update()\n",
    "    \n",
    "    # target network tracks the model\n",
    "    for dst, src in zip(self.target_network.params(), self.model_network.params()):\n",
    "        dst.data = self.tau * src.data + (1 - self.tau) * dst.data\n",
    "\n",
    "    return loss.data\n",
    "\n",
    "QLearningAgent.update_model = update_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_optimizer(self, model, learning_rate):\n",
    "\n",
    "    optimizer = optimizers.SGD(learning_rate)\n",
    "    # optimizer = optimizers.Adam(alpha=learning_rate)\n",
    "    # optimizer = optimizers.AdaGrad(learning_rate)\n",
    "    # optimizer = optimizers.RMSpropGraves(learning_rate, 0.95, self.momentum, 1e-2)\n",
    "\n",
    "    optimizer.setup(model)\n",
    "    return optimizer\n",
    "\n",
    "QLearningAgent.init_optimizer = init_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Implements basic replay memory\"\"\"\n",
    "\n",
    "    def __init__(self, observation_size, max_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.num_observed = 0\n",
    "        self.max_size = max_size\n",
    "        self.samples = {\n",
    "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
    "                                       dtype=np.float32).reshape(self.max_size, 1, self.observation_size),\n",
    "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
    "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "               }\n",
    "\n",
    "    def observe(self, state, action, reward, done):\n",
    "        index = self.num_observed % self.max_size\n",
    "        self.samples['obs'][index, :] = state\n",
    "        self.samples['action'][index, :] = action\n",
    "        self.samples['reward'][index, :] = reward\n",
    "        self.samples['terminal'][index, :] = done\n",
    "        \n",
    "        self.num_observed += 1\n",
    "        \n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        max_index = min(self.num_observed, self.max_size) - 1\n",
    "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
    "        \n",
    "        s      = Variable(np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32))\n",
    "        s_next = Variable(np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32))\n",
    "\n",
    "        a      = Variable(self.samples['action'][sampled_indices].reshape(minibatch_size))\n",
    "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
    "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
    "\n",
    "        return (s, a, r, s_next, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: DQN on SimpleRooms\n",
    "\n",
    "It's time to test your DQN implementation. Are you ready? The experiment on the SimpleRoom task below is a good test case. The task can be learned within less than 5000 steps. If your learning curve stays flat - something is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_env = SimpleRooms()\n",
    "\n",
    "simple_q_agent = QLearningAgent(\n",
    "    simple_env.action_space.actions,\n",
    "    16, # observation size\n",
    "    nhidden = 512,\n",
    "    epsilon = 1.,\n",
    "    mem_size = 10000,\n",
    "    learning_rate = .5,\n",
    "    tau = .001,\n",
    "    minibatch_size = 32,\n",
    "    epoch_length = 100)\n",
    "simple_q_experiment = Experiment(simple_env, simple_q_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_q_experiment.run(5000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: DQN Agent on MineRL Navigation\n",
    "\n",
    "Now we're ready to test our DQN agent on our discretized Minecraft Navigation task. Again, if everything is implemented correctly, reward should go up within less than 3000 training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wrapped_env = DiscreteMinecraftEnvWrapper(nav_env)\n",
    "minerl_q_agent = QLearningAgent(\n",
    "    wrapped_env.action_space.actions,\n",
    "    28, # observation size\n",
    "    nhidden = 512,\n",
    "    epsilon = 1.,\n",
    "    mem_size = 10000,\n",
    "    learning_rate = .5,\n",
    "    tau = .001,\n",
    "    minibatch_size = 32,\n",
    "    epoch_length = 100)\n",
    "minerl_q_experiment = Experiment(wrapped_env, minerl_q_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minerl_q_experiment.run(3000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "- Update / clean up reward visualization\n",
    "- Update SimpleRooms rendering\n",
    "- Remove solution code from student version\n",
    "- Complete text / instructions\n",
    "- Add conclusion - next steps: additional experiments to run\n",
    "- Get feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
